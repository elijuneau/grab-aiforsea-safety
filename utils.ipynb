{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT RECOMMENDED! ignore warnings\n",
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Correlation summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleModelSearch:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "        1. Review hyperparameters grid. \n",
    "        3. Add average training AUC\n",
    "    \n",
    "    Search for the best model type using cross validation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter: int\n",
    "        Number of parameter settings that are sampled.\n",
    "    cv: int\n",
    "        Number of cross validation folds.\n",
    "    scoring: string, default='roc_auc'\n",
    "        Scoring metrics (e.g: AUC, Accuracy, F1, etc)\n",
    "    n_jobs: int, default=None\n",
    "        Number of thread to use for parallel computing.\n",
    "    random_state: int, default=None\n",
    "        Random seed for RandomizedSearchCV().\n",
    "        Set seed to integer to get reproducible result.\n",
    "    verbose: boolean, default=False\n",
    "        Set to True to print messages when searching in progress.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    model_search_params_: dict\n",
    "        Contains all the optimal CV parameters for every searched model.\n",
    "    model_search_score_: DataFrame\n",
    "        Contains all the best CV score for every searched model.\n",
    "    best_model_: dict\n",
    "        Contains the name, parameters and score of best model.\n",
    "    \"\"\"\n",
    "    _model = {\n",
    "        'logistic': LogisticRegression(max_iter=1000),\n",
    "        'lda': LinearDiscriminantAnalysis(), \n",
    "        'naivebayes': GaussianNB(),\n",
    "        'rf': RandomForestClassifier(n_estimators=100),\n",
    "        'xgboost': XGBClassifier(),\n",
    "        'knn': KNeighborsClassifier(),\n",
    "        'svc': SVC(kernel='rbf'),\n",
    "        'mlp': MLPClassifier(max_iter=500)\n",
    "    }\n",
    "    \n",
    "    _params_grid = {\n",
    "        'logistic': {\n",
    "            'solver' : ['liblinear', 'saga'],\n",
    "            'C' : [1e-3, 1e-2, 0.1, 1, 10, 100]\n",
    "        },\n",
    "        'rf': {\n",
    "            'max_depth': randint(10, 100),\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'min_samples_leaf': randint(1, 4),\n",
    "            'min_samples_split': randint(2, 10),\n",
    "            'bootstrap': [True, False]\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'max_depth': randint(1,6),\n",
    "            'min_child_weight': randint(0,6),\n",
    "            'subsample': uniform(loc=0.6, scale=0.4),\n",
    "            'colsample_bytree': uniform(loc=0.6, scale=0.4),\n",
    "            'gamma': [i/10.0 for i in range(0,5)],\n",
    "            'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "        }, \n",
    "        'knn': {\n",
    "            'n_neighbors': randint(1, 100)\n",
    "        },\n",
    "        'svc': {\n",
    "            'gamma': [0.1, 1, 10, 100],\n",
    "            'C': [0.1, 1, 10, 100, 1000]\n",
    "        },\n",
    "        'mlp': {\n",
    "            'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "            'activation': ['tanh', 'relu'],\n",
    "            'solver': ['sgd', 'adam'],\n",
    "            'alpha': [0.0001, 0.05],\n",
    "            'learning_rate': ['constant','adaptive']\n",
    "        }\n",
    "    }\n",
    "    def __init__(self, n_iter, cv, scoring='roc_auc', n_jobs=None,  \n",
    "                 random_state=None, verbose=False):\n",
    "        self.n_iter = n_iter\n",
    "        self.scoring = scoring\n",
    "        self.n_jobs = n_jobs\n",
    "        self.cv = cv\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def _get_params_score(self, X, y, model):\n",
    "        \"\"\"\n",
    "        Find the optimal CV parameters and score for each model types.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: DataFrame\n",
    "            Input features\n",
    "        Y: DataFrame\n",
    "            Input label\n",
    "        model: string\n",
    "            Name of the model to be tested. (See class variables)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        params: dict\n",
    "            Best CV model parameters.\n",
    "        score: int\n",
    "            Best CV score. \n",
    "        \"\"\"\n",
    "        \n",
    "        # valid model name that has searching grid\n",
    "        if model in self._params_grid.keys():\n",
    "            rand_search = RandomizedSearchCV(\n",
    "                estimator = self._model[model],\n",
    "                param_distributions = self._params_grid[model],\n",
    "                scoring = self.scoring,\n",
    "                n_iter = self.n_iter,\n",
    "                cv = self.cv,\n",
    "                n_jobs = self.n_jobs,\n",
    "                random_state = self.random_state)\n",
    "            \n",
    "            rand_search.fit(X, y)\n",
    "            \n",
    "            return rand_search.best_params_, rand_search.best_score_\n",
    "        # valid model name without searching grid\n",
    "        elif model in self._model.keys():\n",
    "            cv_fit = cross_validate(self._model[model], X, y, \n",
    "                                    scoring=self.scoring, cv=self.cv, \n",
    "                                    n_jobs=self.n_jobs)\n",
    "            \n",
    "            return self._model[model].get_params(), cv_fit['test_score'].mean()\n",
    "        else:\n",
    "            raise Exception('Invalid model! Please input a valid model name')\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: DataFrame\n",
    "            Input features\n",
    "        Y: DataFrame\n",
    "            Input label\n",
    "        \"\"\"\n",
    "        # declare dtype for desired output\n",
    "        self.model_search_params_ = {}\n",
    "        self.model_search_score_ = pd.DataFrame([], columns=['model', 'score'])\n",
    "        self.best_model_ = {}\n",
    "        \n",
    "        for model in self._model.keys():\n",
    "            if self.verbose == True:\n",
    "                print(\"Currently testing for {}\".format(model))\n",
    "                \n",
    "            # find the CV results and optimal params\n",
    "            params, score= self._get_params_score(X, y, model)\n",
    "            \n",
    "            # record best CV params & score\n",
    "            self.model_search_params_[model] = params\n",
    "            self.model_search_score_ = self.model_search_score_.\\\n",
    "                                       append({'model': model, 'score': score}, \n",
    "                                              ignore_index=True)\n",
    "        \n",
    "        # sort search results in descending order\n",
    "        self.model_search_score_ = self.model_search_score_.\\\n",
    "                                   sort_values(by='score', ascending=False).\\\n",
    "                                   reset_index(drop=True)\n",
    "        \n",
    "        # extract the name, params and score for best model\n",
    "        best_model = self.model_search_score_.\\\n",
    "                     loc[self.model_search_score_.score.idxmax(), :]\n",
    "        \n",
    "        self.best_model_['name'] = best_model.model\n",
    "        self.best_model_['score'] = best_model.score\n",
    "        self.best_model_['params'] = self.model_search_params_[best_model.model]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendedModelSearch(SingleModelSearch):\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        1. Suggest candidate models (Check correlation - choose uncorrelated)\n",
    "        2. Add average training AUC\n",
    "    \n",
    "    Search for the candidate models for blending using cross validation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    frac: float, range within (0, 1]\n",
    "        Proportion of features that are sampled.\n",
    "    n_experiments:\n",
    "        Number of times to repeat the sampling plus searching process.\n",
    "    n_iter: int\n",
    "        Number of parameter settings that are sampled.\n",
    "    cv: int\n",
    "        Number of cross validation folds.\n",
    "    scoring: string, default='roc_auc'\n",
    "        Scoring metrics (e.g: AUC, Accuracy, F1, etc)\n",
    "    n_jobs: int, default=None\n",
    "        Number of thread to use for parallel computing.\n",
    "    random_state: int, default=None\n",
    "        Random seed for RandomizedSearchCV() and features sampling.\n",
    "        Set seed to integer to get reproducible result.\n",
    "    verbose: boolean, default=False\n",
    "        Set to True to print messages when searching in progress.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    model_search_params_: dict\n",
    "        Contains all the optimal CV parameters for every searched model.\n",
    "    model_search_score_: DataFrame\n",
    "        Contains all the best CV score for every searched model.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, frac, n_experiment, n_iter, cv, scoring='roc_auc', \n",
    "                 n_jobs=None, random_state=None, verbose=False):\n",
    "        super().__init__(n_iter=n_iter, cv=cv, scoring=scoring, n_jobs=n_jobs,\n",
    "                         random_state=random_state, verbose=verbose)\n",
    "        self.frac = frac\n",
    "        self.n_experiment = n_experiment\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: DataFrame\n",
    "            Input features\n",
    "        Y: DataFrame\n",
    "            Input label\n",
    "        \"\"\"\n",
    "        \n",
    "        # declare dtype for the desired output\n",
    "        self.model_search_params_ = {}\n",
    "        self.model_search_score_ = pd.DataFrame(\n",
    "            [], \n",
    "            columns=['experiment', 'model', 'score'])\n",
    "        \n",
    "        for i in range(self.n_experiment):\n",
    "            \n",
    "            # sample the features randomly\n",
    "            X_sample = X.sample(frac=self.frac, axis=1,\n",
    "                               random_state=self.random_state + i)\n",
    "            y_sample = y\n",
    "            \n",
    "            # test for all model types\n",
    "            for model in self._model.keys():\n",
    "                if self.verbose == True:\n",
    "                    print(\"Experiment {}: Testing for {}\".format(i, model))\n",
    "                \n",
    "                # find the CV results and optimal params\n",
    "                params, score = super()._get_params_score(\n",
    "                    X_sample, y_sample, model)\n",
    "                \n",
    "                # save the searched parameters\n",
    "                self.model_search_params_[(i, model)] = \\\n",
    "                    {'params': params, 'feature': list(X_sample.columns)}\n",
    "                \n",
    "                # save the search results\n",
    "                self.model_search_score_ = \\\n",
    "                    self.model_search_score_.append(\n",
    "                        {'experiment': i, 'model': model, 'score': score},\n",
    "                         ignore_index=True)\n",
    "        \n",
    "        # sort the search results in descending order\n",
    "        self.model_search_score_ = self.model_search_score_.\\\n",
    "                                   sort_values(by='score', ascending=False)\\\n",
    "                                   .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Software Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('feat.csv', encoding = \"ISO-8859-1\", low_memory=False)\n",
    "# data = data.loc[:, ['AGE', 'DEPENDENT_CNT', 'DELQ_FLG']].dropna().reset_index(drop=True)\n",
    "# data['DELQ_FLG'] = data['DELQ_FLG'].map({'N': 0, 'Y': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently testing for mlp\n",
      "Currently testing for naivebayes\n",
      "Currently testing for lda\n",
      "Currently testing for svc\n",
      "Currently testing for logistic\n",
      "Currently testing for knn\n",
      "Currently testing for rf\n",
      "Currently testing for xgboost\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lda</td>\n",
       "      <td>0.715709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.714227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.711938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>naivebayes</td>\n",
       "      <td>0.695930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.688711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logistic</td>\n",
       "      <td>0.684187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.608873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svc</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model     score\n",
       "0         lda  0.715709\n",
       "1          rf  0.714227\n",
       "2     xgboost  0.711938\n",
       "3  naivebayes  0.695930\n",
       "4         knn  0.688711\n",
       "5    logistic  0.684187\n",
       "6         mlp  0.608873\n",
       "7         svc  0.500000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test single model mode\n",
    "msearch = SingleModelSearch(n_iter=1, cv=3, verbose=True, n_jobs=-1)\n",
    "msearch.fit(data.loc[:, ~data.columns.isin(['bookingID', 'label'])], data['label'])\n",
    "msearch.model_search_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: Testing for mlp\n",
      "Experiment 0: Testing for naivebayes\n",
      "Experiment 0: Testing for lda\n",
      "Experiment 0: Testing for svc\n",
      "Experiment 0: Testing for logistic\n",
      "Experiment 0: Testing for knn\n",
      "Experiment 0: Testing for rf\n",
      "Experiment 0: Testing for xgboost\n",
      "Experiment 1: Testing for mlp\n",
      "Experiment 1: Testing for naivebayes\n",
      "Experiment 1: Testing for lda\n",
      "Experiment 1: Testing for svc\n",
      "Experiment 1: Testing for logistic\n",
      "Experiment 1: Testing for knn\n",
      "Experiment 1: Testing for rf\n",
      "Experiment 1: Testing for xgboost\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>model</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.732086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>lda</td>\n",
       "      <td>0.731074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>lda</td>\n",
       "      <td>0.729299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.728737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.726572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.723179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.718343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>rf</td>\n",
       "      <td>0.699245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.691859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>naivebayes</td>\n",
       "      <td>0.684351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>naivebayes</td>\n",
       "      <td>0.681124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.673139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>mlp</td>\n",
       "      <td>0.668544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>mlp</td>\n",
       "      <td>0.650303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>svc</td>\n",
       "      <td>0.501333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>svc</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment       model     score\n",
       "0           0    logistic  0.732086\n",
       "1           0         lda  0.731074\n",
       "2           1         lda  0.729299\n",
       "3           1     xgboost  0.728737\n",
       "4           1    logistic  0.726572\n",
       "5           0     xgboost  0.723179\n",
       "6           1          rf  0.718343\n",
       "7           0          rf  0.699245\n",
       "8           0         knn  0.691859\n",
       "9           1  naivebayes  0.684351\n",
       "10          0  naivebayes  0.681124\n",
       "11          1         knn  0.673139\n",
       "12          1         mlp  0.668544\n",
       "13          0         mlp  0.650303\n",
       "14          0         svc  0.501333\n",
       "15          1         svc  0.500000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test blended mode\n",
    "msearch = BlendedModelSearch(frac=0.5, n_experiment=2, n_iter=5, \n",
    "                             cv=3, random_state=89,\n",
    "                             n_jobs=-1,verbose=True)\n",
    "msearch.fit(data.loc[:, ~data.columns.isin(['bookingID', 'label'])], data['label'])\n",
    "msearch.model_search_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
