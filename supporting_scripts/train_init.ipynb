{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembled Modelling\n",
    "**(Initial Stage)**: Identify strong and weak learners using cross validation.  <br>\n",
    "**(STAGE 1 Ensemble)**: Check correlation between predictions made by initial stage models. Choose only models with low correlation as candidate models. <br>\n",
    "**(STAGE 2 Ensemble)**: Train the weak learners. Average their outputs and use it as new features to train the strong learners. <br>\n",
    "**(STAGE 3 Ensemble)**: Use the output from Stage 2 stong learners to train a final 'meta-learner' model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import build_feature\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed-train.csv is output from build_feature() function\n",
    "features = pd.read_csv('data/preprocessed-train.csv', index_col = 'bookingID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('data/ori_labels.csv', index_col = 'bookingID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Initial Stage): Identify Strong and Weak Learners Using Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models that we want to test\n",
    "model = {\n",
    "    'logistic': LogisticRegression(max_iter=500),\n",
    "    'lda': LinearDiscriminantAnalysis(), \n",
    "    'svc': SVC(kernel='rbf'),\n",
    "    'naivebayes': GaussianNB(),\n",
    "    'rf': RandomForestClassifier(n_estimators=100),\n",
    "    'xgboost': XGBClassifier(),\n",
    "    'mlp': MLPClassifier(max_iter=500)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters grid for models that we want to test\n",
    "model_params = {\n",
    "    'logistic': {\n",
    "        'solver' : ['liblinear', 'saga'],\n",
    "        'C' : [1e-3, 1e-2, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'lda': {\n",
    "        'solver': ['svd', 'lsqr'],\n",
    "         'tol': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "    },\n",
    "    'svc': {\n",
    "        'gamma': [0.1, 1, 10, 100],\n",
    "        'C': [0.1, 1, 10, 100, 1000]\n",
    "    },\n",
    "    'naivebayes': {\n",
    "        'var_smoothing': [1e-11, 1e-10, 1e-09, 1e-08, 1e-7]\n",
    "    },\n",
    "    'rf': {\n",
    "        'max_depth': randint(10, 100),\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'min_samples_leaf': randint(1, 4),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'max_depth': randint(1,6),\n",
    "        'min_child_weight': randint(0,6),\n",
    "        'subsample': uniform(loc=0.6, scale=0.4),\n",
    "        'colsample_bytree': uniform(loc=0.6, scale=0.4),\n",
    "        'gamma': [i/10.0 for i in range(0,5)],\n",
    "        'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "    }, \n",
    "    'mlp': {\n",
    "        'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant','adaptive']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Strong / Weak Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently computing for lda\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Currently computing for logistic\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Currently computing for mlp\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Currently computing for xgboost\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Currently computing for rf\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Currently computing for svc\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Currently computing for naivebayes\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Training Done! Time Used: 5241.812428474426\n"
     ]
    }
   ],
   "source": [
    "model_config = {}\n",
    "\n",
    "col_sample = 0.7\n",
    "n_init = 5\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# for every models\n",
    "for key in model:\n",
    "    print(\"Currently computing for\", key)\n",
    "    \n",
    "    best_match = {}\n",
    "    \n",
    "    # sample (col_sample %) of the columns randomly (n_init) times\n",
    "    for i in range(n_init):\n",
    "        print('Iteration', i + 1)\n",
    "        # sample 70% of the columns randomly\n",
    "        sample_feat = features.sample(frac=col_sample, axis=1)\n",
    "        sample_label = pd.merge(sample_feat.reset_index(), y, on='bookingID')['label']\n",
    "        \n",
    "        # randomized search through the hyperparameters grid\n",
    "        rand_search = RandomizedSearchCV(estimator = model[key], \n",
    "                                         param_distributions=model_params[key], \n",
    "                                         scoring='roc_auc', \n",
    "                                         n_iter=5, \n",
    "                                         iid=False, \n",
    "                                         cv=5, \n",
    "                                         n_jobs=-1)\n",
    "        rand_search.fit(sample_feat, sample_label)\n",
    "        \n",
    "        # update the best model's hyperparameters and columns used\n",
    "        if i == 0:\n",
    "            best_match['model'] = key\n",
    "            best_match['col_names'] = sample_feat.columns\n",
    "            best_match['hyperparams'] = rand_search.best_params_\n",
    "            best_match['roc'] = rand_search.best_score_\n",
    "        elif rand_search.best_score_ > best_match['roc']:\n",
    "            best_match['model'] = key\n",
    "            best_match['col_names'] = sample_feat.columns\n",
    "            best_match['hyperparams'] = rand_search.best_params_\n",
    "            best_match['roc'] = rand_search.best_score_\n",
    "    \n",
    "    model_config[key] = best_match\n",
    "    \n",
    "print('Training Done! Time Used:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Time: 87 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
