{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembled Modelling\n",
    "**(STAGE 2 Ensemble)**: Average outputs of weak learners and use it as new features to train the strong learners. <br>\n",
    "**(STAGE 3 Ensemble)**: Use the output from Stage 2 stong learners to train a final 'meta-learner' model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('../data/preprocessed-train.csv', index_col = 'bookingID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('../data/ori_labels.csv', index_col = 'bookingID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.merge(features.reset_index(), y, on='bookingID')['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weak learners\n",
    "with open(\"../model_weights/mlp.dat\", \"rb\") as f:  \n",
    "    mlp  = pickle.load(f)\n",
    "\n",
    "with open(\"../model_weights/naivebayes.dat\", \"rb\") as f:  \n",
    "    naivebayes  = pickle.load(f)\n",
    "\n",
    "with open(\"../model_weights/rf.dat\", \"rb\") as f:  \n",
    "    rf  = pickle.load(f)\n",
    "    \n",
    "with open('../model_weights/model-config.pkl', 'rb') as f:  \n",
    "    model_config = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average predictions from weak learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = mlp.predict_proba(features[model_config['mlp']['col_names']])[:, 1]\n",
    "temp2 = naivebayes.predict_proba(features[model_config['naivebayes']['col_names']])[:, 1]\n",
    "avg = (temp1 + temp2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Train Strong Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['avg'] = avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of parameters for XGBoost\n",
    "params_logistic = {\n",
    "    'solver' : ['liblinear', 'saga'],\n",
    "    'C' : [1e-3, 1e-2, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "params_lda = {\n",
    "    'solver': ['svd', 'lsqr'],\n",
    "    'tol': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "\n",
    "params_xgboost = {\n",
    "    'max_depth': randint(1,6),\n",
    "    'min_child_weight': randint(0,6),\n",
    "    'subsample': uniform(loc=0.6, scale=0.4),\n",
    "    'colsample_bytree': uniform(loc=0.6, scale=0.4),\n",
    "    'gamma': [i/10.0 for i in range(0,5)],\n",
    "    'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 12 is smaller than n_iter=30. Running 12 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   52.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.42 s, sys: 96 ms, total: 4.52 s\n",
      "Wall time: 1min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'C': 100, 'solver': 'liblinear'}, 0.7912019246889356)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search_logistic = RandomizedSearchCV(estimator = LogisticRegression(), \n",
    "                                 param_distributions=params_logistic, \n",
    "                                 scoring='roc_auc', \n",
    "                                 n_iter=30, \n",
    "                                 iid=False, \n",
    "                                 cv=5, \n",
    "                                 n_jobs=-1,\n",
    "                                 verbose=True)\n",
    "\n",
    "%time rand_search_logistic.fit(features[model_config['logistic']['col_names'] + ['avg']], label)\n",
    "rand_search_logistic.best_params_, rand_search_logistic.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 10 is smaller than n_iter=30. Running 10 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    5.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 532 ms, sys: 52 ms, total: 584 ms\n",
      "Wall time: 6.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'solver': 'svd', 'tol': 1e-06}, 0.8122928250684665)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search_lda = RandomizedSearchCV(estimator = LinearDiscriminantAnalysis(), \n",
    "                                 param_distributions=params_lda, \n",
    "                                 scoring='roc_auc', \n",
    "                                 n_iter=30, \n",
    "                                 iid=False, \n",
    "                                 cv=5, \n",
    "                                 n_jobs=-1,\n",
    "                                 verbose=True)\n",
    "\n",
    "%time rand_search_lda.fit(features[model_config['lda']['col_names'] + ['avg']], label)\n",
    "rand_search_lda.best_params_, rand_search_lda.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.92 s, sys: 132 ms, total: 6.06 s\n",
      "Wall time: 4min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.8930552389188486,\n",
       "  'gamma': 0.2,\n",
       "  'max_depth': 5,\n",
       "  'min_child_weight': 3,\n",
       "  'reg_alpha': 1e-05,\n",
       "  'subsample': 0.7471291925293697},\n",
       " 0.9110989009814426)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search_xgboost = RandomizedSearchCV(estimator = XGBClassifier(), \n",
    "                                 param_distributions=params_xgboost, \n",
    "                                 scoring='roc_auc', \n",
    "                                 n_iter=30, \n",
    "                                 iid=False, \n",
    "                                 cv=5, \n",
    "                                 n_jobs=-1,\n",
    "                                 verbose=True)\n",
    "\n",
    "%time rand_search_xgboost.fit(features[model_config['xgboost']['col_names'] + ['avg']], label)\n",
    "rand_search_xgboost.best_params_, rand_search_xgboost.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrain full dataset using BEST model and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = LogisticRegression(**rand_search_logistic.best_params_, max_iter=500)\n",
    "logistic.fit(features[model_config['logistic']['col_names'] + ['avg']], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=1e-06)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(**rand_search_lda.best_params_)\n",
    "lda.fit(features[model_config['lda']['col_names'] + ['avg']], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8930552389188486,\n",
       "              gamma=0.2, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=3, missing=None, n_estimators=100, n_jobs=-1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=1e-05, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=0.7471291925293697, verbosity=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost = XGBClassifier(\n",
    "    n_estimators = 100, #rand_search.best_params_['n_estimators'],\n",
    "    learning_rate = 0.1, #rand_search.best_params_['learning_rate'],\n",
    "    **rand_search_xgboost.best_params_,\n",
    "    n_jobs = -1)\n",
    "\n",
    "xgboost.fit(features[model_config['xgboost']['col_names'] + ['avg']], label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Strong Learner Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model_weights/lda_rf.dat', 'wb') as f:  \n",
    "    pickle.dump(lda, f)\n",
    "    \n",
    "with open('../model_weights/xgboost_rf.dat', 'wb') as f:  \n",
    "    pickle.dump(xgboost, f)\n",
    "    \n",
    "with open('../model_weights/logistic_rf.dat', 'wb') as f:  \n",
    "    pickle.dump(logistic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 3: Train Meta Learner Using Output From Strong Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output From Strong Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = logistic.predict_proba(features[model_config['logistic']['col_names'] + ['avg']] )[:, 1]\n",
    "temp4 = lda.predict_proba(features[model_config['lda']['col_names'] + ['avg']])[:, 1]\n",
    "temp5 = xgboost.predict_proba(features[model_config['xgboost']['col_names'] + ['avg']])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if overfitted\n",
    "print(metrics.roc_auc_score(label, temp3))\n",
    "print(metrics.roc_auc_score(label, temp4))\n",
    "print(metrics.roc_auc_score(label, temp5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([])\n",
    "df['logistic'] = temp3\n",
    "df['lda'] = temp4\n",
    "df['xgboost'] = temp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 72 ms, total: 1.94 s\n",
      "Wall time: 1min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.6928004622825488,\n",
       "  'gamma': 0.0,\n",
       "  'max_depth': 4,\n",
       "  'min_child_weight': 0,\n",
       "  'reg_alpha': 0.1,\n",
       "  'subsample': 0.7219384230404164},\n",
       " 0.9606306735501423)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search_xgboost2 = RandomizedSearchCV(estimator = XGBClassifier(), \n",
    "                                 param_distributions=params_xgboost, \n",
    "                                 scoring='roc_auc', \n",
    "                                 n_iter=30, \n",
    "                                 iid=False, \n",
    "                                 cv=5, \n",
    "                                 n_jobs=-1,\n",
    "                                 verbose=True)\n",
    "\n",
    "%time rand_search_xgboost2.fit(df, label)\n",
    "rand_search_xgboost2.best_params_, rand_search_xgboost2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8930552389188486,\n",
       "              gamma=0.2, learning_rate=0.01, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=3, missing=None, n_estimators=100, n_jobs=-1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=1e-05, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=0.7471291925293697, verbosity=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost2 = XGBClassifier(\n",
    "    n_estimators = 100, #rand_search.best_params_['n_estimators'],\n",
    "    learning_rate = 0.01, #rand_search.best_params_['learning_rate'],\n",
    "    **rand_search_xgboost.best_params_,\n",
    "    n_jobs = -1)\n",
    "\n",
    "xgboost2.fit(df, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model_weights/meta_rf.dat', 'wb') as f:  \n",
    "    pickle.dump(xgboost2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
